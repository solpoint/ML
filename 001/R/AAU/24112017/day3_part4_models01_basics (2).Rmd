---
output: html_document
---

# Introduction

* Graphics: Does not scale
* Model: Scales

Modelling:

* Prediction
* Explanation / inference

> All models are wrong but some are useful (George Box, 1976)

Ideally: model capture true "signals" and ignore "noise".

R4DS:

* "predictive" models vs "data discovery" models
* supervised and unsupervised

* Not much math, refer to e.g. ISL (or some other reference)

Model assesment: Difficult. R4DS: qualitative assessment and natural scepticism. 
Not quantitatively assessing models.


## Hypothesis generation vs. hypothesis confirmation

* Traditionally, the focus of modelling is on inference: testing a hypothesis. Important points:
    + Each observation can either be used for exploration OR confirmation, not both.
        - Exploration: Use observation many times.
        - Confirmation: Only once.
    + Use an observation twice: going from confirmation to exploration.
    + To test a hypothesis: use data independent of the data that you used to generate the hypothesis.
    + Otherwise overly optimistic.
    + Never sell an exploratory analysis as a confirmatory analysis: fundamentally misleading. 

Confirmatory analysis: one approach is to split your data into three pieces before you begin the analysis:

1.  60% of your data goes into a __training__ (or exploration) set. You're 
    allowed to do anything you like with this data: visualise it and fit tons 
    of models to it.
  
2.  20% goes into either
    + a __query__ set: compare models or visualisations by hand, but you're not allowed to use it as part of
    an automated process.
    + a __tuning__ set: determine tuning parameters, e.g. penalty in LASSO

3.  20% is held back for a __test__ set. You can only use this data ONCE, to 
    test your final model. To test the hypothesis ("calculate a $p$ value").

# Model basics

Start with only simulated datasets. 
Simple, and not at all interesting, but help understand the essence of modelling.

There are two parts to a model:

1.  __Family of models__: 
    + `y = a_1 * x + a_2` 
    + `y = a_1 * x^a_2`
    + `x` and `y`: data (variables from data)
    + `a_1` and `a_2`: parameters that can vary to capture different patterns

2.  __Fitted model__: Estimate parameters. 
    + `y = 3 * x + 7`
    + `y = 9 * x^2`
    + "Closest" model from a family of models
    + Don't know if model is good model and it certainly doesn't imply that the model is "true".
    
George Box puts this well in his famous aphorism:

> All models are wrong, but some are useful.

Also:

> "Is the model illuminating and useful?".

The goal of a model is not to uncover full truth (often impossible), but to discover a simple approximation that is still useful. 

### Prerequisites

```{r setup, message = FALSE}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

## A simple model

Simulated dataset `sim1` (from __modelr__ package):

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

`y = a_0 + a_1*x`

```{r}
set.seed(1)
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)
models

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point() 
```

250 models: many are really bad! 

"Closest" model: is "close" to the data. Quantify distance between data and model.

Then find the value of `a_0` and `a_1` that generate the model with the smallest distance from this data.

Distance between data and model: Vertical distance between each point and the model: (Note that the $x$ values are shifted slightly to see the individual distances.)

```{r, echo = FALSE}
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, colour = "grey40") +
  geom_point(colour = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "#3366FF") 
```

Distances: difference between the y value given by the model (the __prediction__), and the actual y value in the data (the __response__).

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)
```

Compute an overall distance: "root-mean-squared deviation" (lots of appealing mathematical properties):

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)
```

__purrr__:

```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

models
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

Models: observations. Visualising with a scatterplot of `a1` vs `a2`:

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

Lots of random models vs evenly spaced grid of points (this is called a grid search):

```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

Iteratively: finer and finer grid.

But: clever optimisation methods / numerical minimisation (gradient descent, Newton, ...). In R, we can do that with `optim()`:

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

```{r}
sim1_mod = lm(y ~ x, sim1)
coef(sim1_mod)
confint(sim1_mod)
```

Standard errors: Not covered here. For some models, ok -- others difficult (bootstrapping).

`lm()` does NOT use `optim()` (uses special structure of linear models)

### Exercises

R4DS Exercises p. 353-354 (online 23.2.1 Exercises).


## Visualising models

* Simple models: understand model by considering model family and the fitted coefficients.
* Now: focus on understanding a model by looking at its predictions and residuals (subtracting the predictions from the observations).

### Predictions

Visualise the predictions from a model:

Generate evenly spaced grid of values that covers the region where our data lies: `modelr::data_grid()`:

```{r}
grid <- sim1 %>% 
  data_grid(x) 
grid
```

(More interesting with more variables.)

Add predictions: `modelr::add_predictions()` that takes a data frame and a model.

```{r}
grid <- grid %>% 
  add_predictions(sim1_mod) 
grid
```

(Also possible to add predictions to original dataset.)

Plot the predictions

Above process works for _any_ model in R, not just `geom_abline()`. See also <http://vita.had.co.nz/papers/model-vis.html>.

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)
```

### Residuals

"Flip-side" / duality of predictions: __residuals__ (distances between the observed and predicted values):

* Pattern that the model has captured
* What the model has missed

Add residuals: `add_residuals()` (works much like `add_predictions()`), except original dataset used as observed $y$ values are needed.

```{r}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)
sim1
```

Ways to understand what the residuals tell us about the model:

1. Simply draw a frequency polygon to help us understand the distribution / spread of the residuals:

```{r}
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```

```{r}
nbins = nclass.FD(sim1 %>% pull(resid)) # nclass.scott() / nclass.Sturges()

ggplot(sim1, aes(resid)) + 
  geom_freqpoly(bins = nbins)

ggplot(sim1, aes(resid)) + 
  geom_histogram(bins = nbins)
```

How far away are the predictions from the observed values? 
R4DS: "Note that the average of the residual will always be 0." Yes, for some models.

```{r}
sim1 %>% add_residuals(lm(y ~ x, sim1)) %>% summarise(mean(resid))
sim1 %>% add_residuals(lm(y ~ x - 1, sim1)) %>% summarise(mean(resid))
```

Often plotting residuals are more information than the original predictor. 

```{r}
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 
```

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = sim1 %>% add_predictions(lm(y ~ x - 1, sim1)), colour = "blue", size = 1) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)

ggplot(sim1 %>% add_residuals(lm(y ~ x - 1, sim1)), aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 
```


This looks like random noise, suggesting that our model has done a good job of capturing the patterns in the dataset.

"The model captured all structure that we could hope for by using $x$."

### Exercises

R4DS Exercises p. 358 (online 23.3.3 Exercises).


## Formulas and model families

Formulas: `lm()`, `facet_wrap()`, `facet_grid()`, ...

General way of getting "special behaviour".

`y ~ x` is translated to `y = a_1 + a_2*x`

```{r}
df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)
model_matrix(df, y ~ x1)
```

Intercept? Often, yes. Else: explicitly drop it with `-1`:

```{r}
model_matrix(df, y ~ x1 - 1)
```

Add more variables to the the model:

```{r}
model_matrix(df, y ~ x1 + x2)
```

This formula notation is sometimes called "Wilkinson-Rogers notation", and was initially described in _Symbolic Description of Factorial Models for Analysis of Variance_, by G. N. Wilkinson and C. E. Rogers <https://www.jstor.org/stable/2346786>.


### Categorical variables

```{r}
df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1
)
df
model_matrix(df, response ~ sex)
```

`sexfemale` column? `sexfemale = 1 - sexmale`.

```{r}
sim2
ggplot(sim2) + 
  geom_point(aes(x, y))

ggplot(sim2) + 
  geom_count(aes(x, y))

ggplot(sim2) + 
  geom_jitter(aes(x, y))

ggplot(sim2) + 
  geom_jitter(aes(x, y), height = 0, width = 0.5)

ggplot(sim2) + 
  geom_hex(aes(x, y))

ggplot(sim2) + 
  geom_point(aes(x, y), alpha = 0.2, size = 3)
```

We can fit a model to it, and generate predictions:

```{r}
model_matrix(~ x, data = sim2) %>% head()
mod2 <- lm(y ~ x, data = sim2)
sim2 %>% group_by(x) %>% summarise(mean(y))
mod2
grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
grid
```

A model with one categorical `x` will predict the mean value for each category. (The mean minimises the root-mean-squared distance.) 

```{r}
ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

Unoberved levels:

```{r, error = TRUE}
tibble(x = "e") %>% 
  add_predictions(mod2)
```

### Interactions (continuous and categorical)

Combining a continuous and a categorical variable? 

```{r}
sim3
ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(colour = x2))
```

There are (at least) two possible models you could fit to this data:

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

`y ~ x1 * x2` is translated to `y ~ x1 + x2 + x1:x2` which is `y = a_0 + a_1 * x1 + a_2 * x2 + a_12 * x1 * x2`. 

Note: `*` introduces both the interaction and the main effects in the model.

ANCOVA

To visualise these models we need two new tricks:

1.  We have two predictors, so we need to give `data_grid()` both variables. 
    It finds all the unique values of `x1` and `x2` and then generates all
    combinations. 
   
2.  To generate predictions from both models simultaneously, we can use 
    `gather_predictions()` which adds each prediction as a row. The
    complement of `gather_predictions()` is `spread_predictions()` which adds 
    each prediction to a new column.
    
Together this gives us:

```{r}
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
grid

sim3 %>% 
  data_grid(x1, x2) %>% 
  spread_predictions(mod1, mod2)
```

We can visualise the results for both models on one plot using facetting:

```{r}
ggplot(sim3, aes(x1, y, colour = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model)
```

Note:

* The model that uses `+` has the same slope for each line, but different intercepts.
* The model that uses `*` has a different slope and intercept for each line.

Exercise: Why? Write up the model equations.

Best model? Look at the residuals.

```{r}
sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, colour = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2)
```


Also, `mod2` has more parameters than `mod1`...


### Interactions (two continuous)

Let's take a look at the equivalent model for two continuous variables. Initially things proceed almost identically to the previous example:

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5), 
    x2 = seq_range(x2, 5) 
  ) %>% 
  gather_predictions(mod1, mod2)
grid
```

`seq_range()` inside `data_grid()`: regularly spaced grid of five values between the minimum and maximum numbers. (Instead of every unique value of `x`.) 

Two useful arguments to `seq_range()`:

*  `pretty = TRUE` will generate a "pretty" sequence, i.e. something that looks
    nice to the human eye. This is useful if you want to produce tables of 
    output:
    
    ```{r}
    seq_range(c(0.0123, 0.923423), n = 5)
    seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)
    ```
    
*   `trim = 0.1` will trim off 10% of the tail values. This is useful if the 
    variables have a long tailed distribution and you want to focus on generating
    values near the center:
    
    ```{r}
    x1 <- rcauchy(100)
    seq_range(x1, n = 5)
    seq_range(x1, n = 5, trim = 0.10)
    seq_range(x1, n = 5, trim = 0.25)
    seq_range(x1, n = 5, trim = 0.50)
    ```
    
*   `expand = 0.1` is in some sense the opposite of `trim()` it expands the 
    range by 10%.
    
    ```{r}
    x2 <- c(0, 1)
    seq_range(x2, n = 5)
    seq_range(x2, n = 5, expand = 0.10)
    seq_range(x2, n = 5, expand = 0.25)
    seq_range(x2, n = 5, expand = 0.50)
    ```

Next let's try and visualise that model. We have two continuous predictors, so you can imagine the model like a 3d surface. We could display that using `geom_tile()`:

```{r}
ggplot(grid, aes(x1, x2)) + 
  geom_tile(aes(fill = pred)) + 
  facet_wrap(~ model)
```

Are models different? Our eyes and brains comparing shades of colour!!

Before: looking at the surface from the top.
Now: look at it from either side, showing multiple slices:

```{r, asp = 1/2}
ggplot(grid, aes(x1, pred, colour = x2, group = x2)) + 
  geom_line() +
  facet_wrap(~ model)

ggplot(grid, aes(x2, pred, colour = x1, group = x1)) + 
  geom_line() +
  facet_wrap(~ model)
```

Interaction: Need to consider both values of `x1` and `x2` simultaneously in order to predict `y`.

Two continuous variables: visualisation is hard!

### Transformations

Transformations inside the model formula:
`log(y) ~ sqrt(x1) + x2` is transformed to `log(y) = a_1 + a_2 * sqrt(x1) + a_3 * x2`

If transformation involves `+`, `*`, `^`, or `-`: wrap it in `I()` (so R doesn't treat it like part of the model specification).

`y ~ x + I(x ^ 2)` -> `y = a_1 + a_2 * x + a_3 * x^2`. 
`y ~ x ^ 2 + x` -> `y ~ x * x + x` -> `y ~ x + x` -> `y ~ x` -> `y = a_1 + a_2 * x`.

Use `model_matrix()` to see exactly what equation `lm()` is fitting:

```{r}
df <- tribble(
  ~y, ~x,
   1,  1,
   2,  2, 
   3,  3
)
model_matrix(df, y ~ x^2 + x)
model_matrix(df, y ~ I(x^2) + x)
```

Transformations: approximate non-linear functions. 
Calculus: Taylor's theorem: approximate any smooth function with an infinite sum of polynomials.

Polynomial function get arbitrarily close to a smooth function by fitting an equation like `y = a_1 + a_2*x + a_3*x^2 + a_4*x^3`. 

Much type, in come `poly()`:

```{r}
model_matrix(df, y ~ poly(x, 2))
```

Polynomials rapidly shoot off to positive or negative infinity outside the range of the data. 

One safer alternative is to use the natural spline, `splines::ns()`.

```{r}
library(splines)
model_matrix(df, y ~ ns(x, 2))
```

Let's see what that looks like when we try and approximate a non-linear function:

```{r}
sim5 <- tibble(
  x = seq(0, 3.5 * pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x))
)

ggplot(sim5, aes(x, y)) +
  geom_point()
```

```{r}
mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)
mod6 <- lm(y ~ poly(x, 5), data = sim5)

grid <- sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% 
  gather_predictions(mod1, mod2, mod3, mod4, mod5, mod6, .pred = "y")

ggplot(sim5, aes(x, y)) + 
  geom_point() +
  geom_line(data = grid, colour = "red") +
  facet_wrap(~ model)

ggplot(sim5, aes(x, y)) + 
  geom_point() +
  geom_line(data = sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 2)) %>% 
  #gather_predictions(mod1, mod2, mod3, mod4, mod5, mod6, .pred = "y"), colour = "red") +
  gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = "y"), colour = "red") +
  facet_wrap(~ model)
```

Extrapolation (outside the range of the data): bad. 


## Missing values

Difficult subject.

Drop?
Replace (impute)?

__mice__ package.

R's default behaviour: silently drop them, but `options(na.action = na.warn)` (run in the prerequisites), makes sure you get a warning.

```{r}
df <- tribble(
  ~x, ~y,
  1, 2.2,
  2, NA,
  3, 3.5,
  4, 8.3,
  NA, 10
)

mod <- lm(y ~ x, data = df)
```

To suppress the warning, set `na.action = na.exclude`:

```{r}
mod <- lm(y ~ x, data = df, na.action = na.exclude)
```

You can always see exactly how many observations were used with `nobs()`:

```{r}
nobs(mod)
```

## Other model families

Linear models: `y = a_1 * x1 + a_2 * x2 + ... + a_n * xn`. Response is continuous (and from -Inf to Inf).

Additional assumptions. Especially important for inference (standard errors etc.). E.g. residuals follows a normal distribution. Independent observations. Etc.

Other model families:

* __Generalised linear models__, e.g. `stats::glm()`. 
    + Extends linear models.
    + Non-continuous responses (e.g. binary data or counts)
  
* __Generalised additive models__, e.g. `mgcv::gam()`.
    + Extends generalised linear models.
    + Incorporate arbitrary smooth functions: `y ~ s(x)` -> `y = f(x)` 
    + `gam()` estimate what $f(x)$ is (subject to some constraints to make the problem tractable).

* __Penalised linear models__, e.g. `glmnet::glmnet()`.
    + Add a penalty term to penalise complex models.
    + Introduce bias to lower variance (bias-variance trade-off) -> generalise better.

* __Robust linear models__, e.g. `MASS:rlm()`.
    + Downweight points that are very far away. 
    + Less sensitive to outliers.
    + Not quite as good when there are no outliers.
  
* __Trees__, e.g. `rpart::rpart()`.
    + Completely different way than linear models.
    + Piece-wise constant model, splitting the data into progressively smaller and smaller pieces. 
    + One tree is not terribly effective.
    + Make a lot: __random forests__ (e.g. `randomForest::randomForest()`) or __gradient boosting machines__ (e.g. `xgboost::xgboost`.)

Similarly from a programming perspective.

Very different from an inference perspective.

Mastering linear models: have idea of what ingredients there are in a model.

Statistician vs data scientist.

