{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALING TO BIG DATA\n",
    "**Exercises for Seminar 1**\n",
    "\n",
    "This notebook shows results for the first exercise in the third seminar of BIG DATA\n",
    "\n",
    "Author: Sospeter A. Oluoch\n",
    "\n",
    "Date: 24-05-18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a57e4d84e771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pip install findspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#import pandas as pd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#import pyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "#pip install findspark\n",
    "#import pandas as pd\n",
    "pyspark\n",
    "#import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q1)** What is the signature for combine given the signatures map: (k1, v1) --> list(k2, v2) and reduce: (k2, list(v2)) --> list(k3, v3)?<br/>\n",
    "*combine(k2, list(v2))  ## MapReduce groups all intermediate pairs with the same key and optionally gives them to combine*\n",
    "\n",
    "\n",
    "**Q2)** If we also use the reducer as combiner, what can we then say about k2, k3, v2, and v3?<br/>\n",
    "*Reusing the reducer as the combiner means that it will process the list v2, from the first mapper, emitting only a single value. The same applies for list(v3), from the second mapper, where a single value is emitted. The reducer function would then be called with each unique key containing a single value from each mapper. The output would still be the same*\n",
    "\n",
    "\n",
    "**Q3)** Assume that for some MapReduce job J the runtimes for its successful mappers were a1, a2, …, am\n",
    "and the runtimes for its successful reducers were b1, b2, …, br.<br/>\n",
    "*a.)* What can be said about the runtime of J? <br/>  &nbsp;&nbsp;&nbsp;<b>J = Σ(a<sub>max</sub> + b<sub>max</sub>)</b>\n",
    "\n",
    "\n",
    "*b.)* Why is this important when we consider stragglers? <br/>\n",
    "*Because stragglers, take a longer time before they eventually complete the job and may be wrongly concluded as either a<sub>max</sub> or b<sub>max</sub>, for mapper or reducer jobs respectively, leading to a misconception that execution time took such a longer period. *\n",
    "\n",
    "\n",
    "**Q4)**Sketch solutions in terms of MapReduce to the following problems:\n",
    "    \n",
    "*a.)* Given a collection of web pages, find the average number of HTML tables for the pages.\n",
    "\n",
    "*<u>Map:</u>*<br/>\n",
    "Input -----> Set of pages (Home, Info, Products, Contacts etc.)\n",
    "\n",
    "Output ----> Convert into another set of data (Key,Value)<br/>\n",
    "<span style=\"margin-left:80px\">(table, 1), (table, 1), (table, 1), (table, 1), (table, 1) ....</span>\n",
    "\n",
    "\n",
    "*<u>Reduce</u>*<br/>\n",
    "Input -----> Output from Map function. Set of tuples.\n",
    "\n",
    "Output-----> Converts into smaller set of tuple <br/>\n",
    "<span style=\"margin-left:80px\">(table, X)  where X is the total number of HTML tables\n",
    "\n",
    "Average number of tables = ReduceOutput / NumberOfPages\n",
    "\n",
    "\n",
    "\n",
    "*b.)* Given a collection of football bets, find for each match the average number of goals the\n",
    "betters expect in the match\n",
    "\n",
    "\n",
    "*c.)* Given a dictionary of all words in some language, find all anagrams (different words made\n",
    "of the same letters, for example bus/sub, stop/post, car/arc, and orchestra/carthorse )\n",
    "\n",
    "\n",
    "\n",
    "**Q5)** Consider your answers to the previous exercise. Can you improve your solutions by using\n",
    "combiners? If yes, sketch how.\n",
    "\n",
    "\n",
    "**Q6)** When Hadoop’s TextInputFormat is used, the byte offset from the beginning of the file is used as\n",
    "key and the text line is used as value. Why didn’t they just use the line number as key?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7)** When HBase physically stores a cell version, it also stores the column name (the “qualifier”)\n",
    "\n",
    "a. Why does it do that?\n",
    "\n",
    "b. Should you as a developer care about that?\n",
    "\n",
    "**Q8)** When you create a new table in HBase, insertion speed can be pretty low at first. Why?\n",
    "\n",
    "\n",
    "**Q9.)** When HBase deletes a row x, it inserts a marker in its flush files telling that x has been deleted.\n",
    "Why doesn’t it just set a flag in x?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10.)** In Hive, external tables can also be partitioned if the user explicitly uses ADD PARTITION. Partition\n",
    "locations can also be ALTERed. How is this useful if you have a lot of, say, log data where you most\n",
    "often only query the newest data?\n",
    "\n",
    "\n",
    "**11.)** When two data sets are bucketed on the same key and have b and nb buckets (n, b ∈ ℤ+),\n",
    "respectively, joins can be done efficiently.\n",
    "a. Why is this only possible when there are b and nb buckets, respectively?\n",
    "b. What if the buckets are stored in sorted order? Can we then join even more efficiently?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
