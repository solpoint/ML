{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "sc = SparkContext(appName='local') \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explain your choice of processing framework briefly.</b>\n",
    "\n",
    "Spark as compared to MapReduce is able to run more faster and effeciently than MapReduce both on memory and disk. This is due to\n",
    "it's ability to perform work on the main worker node and prevent unnecessary I/O disk operations. Fcor iterative programs, it \n",
    "has a low iteration period the first time and is even lower in subsequent iterations compared to Hadoop.It even has a rider on \n",
    "it, in that it can easily output the data on disk if the memory is full.\n",
    "As a programming language, it is also easy to set up and is rich in libraries/APIs making it easier to perform REPL operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "|iata|             airport|            city|state|country|        lat|        long|\n",
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "| 00M|            Thigpen |     Bay Springs|   MS|    USA|31.95376472|-89.23450472|\n",
      "| 00R|Livingston Municipal|      Livingston|   TX|    USA|30.68586111|-95.01792778|\n",
      "| 00V|         Meadow Lake|Colorado Springs|   CO|    USA|38.94574889|-104.5698933|\n",
      "| 01G|        Perry-Warsaw|           Perry|   NY|    USA|42.74134667|-78.05208056|\n",
      "| 01J|    Hilliard Airpark|        Hilliard|   FL|    USA| 30.6880125|-81.90594389|\n",
      "| 01M|   Tishomingo County|         Belmont|   MS|    USA|34.49166667|-88.20111111|\n",
      "| 02A|         Gragg-Wade |         Clanton|   AL|    USA|32.85048667|-86.61145333|\n",
      "| 02C|             Capitol|      Brookfield|   WI|    USA|   43.08751|-88.17786917|\n",
      "| 02G|   Columbiana County|  East Liverpool|   OH|    USA|40.67331278|-80.64140639|\n",
      "| 03D|    Memphis Memorial|         Memphis|   MO|    USA|40.44725889|-92.22696056|\n",
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+--------------------+\n",
      "|Code|         Description|\n",
      "+----+--------------------+\n",
      "| 02Q|       Titan Airways|\n",
      "| 04Q|  Tradewind Aviation|\n",
      "| 05Q| Comlux Aviation, AG|\n",
      "| 06Q|Master Top Linhas...|\n",
      "| 07Q| Flair Airlines Ltd.|\n",
      "| 09Q|      Swift Air, LLC|\n",
      "| 0BQ|                 DCA|\n",
      "| 0CQ|ACM AIR CHARTER GmbH|\n",
      "| 0FQ|Maine Aviation Ai...|\n",
      "| 0GQ|Inter Island Airw...|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load 1987 data\n",
    "basePath = \"C:/Udvikling/edu/BigData/003/Data/\"\n",
    "airports = spark.read.format(\"csv\").load(basePath + \"airports.csv\", header=\"true\",inferSchema='true').show(10)\n",
    "carriers = spark.read.format(\"csv\").load(basePath + \"carriers.csv\", header=\"true\",inferSchema='true').show(10)\n",
    "flights = spark.read.format(\"csv\").load(basePath + \"flights/1987.csv\", header=\"true\",inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flights from JFK to LAX were 1,144\n"
     ]
    }
   ],
   "source": [
    "#How many flights were there from JFK to LAX?\n",
    "\n",
    "flightsJFKLAX = flights.where(\"origin='JFK' and dest='LAX'\").count()\n",
    "print(\"Number of flights from JFK to LAX were {:,}\".format(flightsJFKLAX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|               xy|\n",
      "+-----------------+\n",
      "|9.446699049774669|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'USING column `xy` cannot be resolved on the left side of the join. The left-side columns: [x];'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o816.join.\n: org.apache.spark.sql.AnalysisException: USING column `xy` cannot be resolved on the left side of the join. The left-side columns: [x];\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$88$$anonfun$apply$66.apply(Analyzer.scala:2088)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$88$$anonfun$apply$66.apply(Analyzer.scala:2088)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$88.apply(Analyzer.scala:2087)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$88.apply(Analyzer.scala:2086)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:2086)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$33.applyOrElse(Analyzer.scala:2072)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$33.applyOrElse(Analyzer.scala:2069)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2069)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2068)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:123)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:117)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:102)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3295)\r\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:916)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d7655c6de0b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#dfz = df1.withColumnRenamed(\"sum(ArrDelay)\", \"Sum\").join(df2.withColumnRenamed(\"avg(ArrDelay)\", \"Average\")).show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdfz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"xy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how)\u001b[0m\n\u001b[0;32m    929\u001b[0m                 \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"how should be basestring\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'USING column `xy` cannot be resolved on the left side of the join. The left-side columns: [x];'"
     ]
    }
   ],
   "source": [
    "#What was the sum and average of all arrival delays for all delayed flights?\n",
    "\n",
    "\n",
    "df1 = flights.select(sum(\"ArrDelay\").alias (\"x\"))\n",
    "df2 = flights.select(avg(\"ArrDelay\").alias (\"xy\"))\n",
    "\n",
    "df2.show()\n",
    "#dfz = df1.withColumnRenamed(\"sum(ArrDelay)\", \"Sum\").join(df2.withColumnRenamed(\"avg(ArrDelay)\", \"Average\")).show()\n",
    "dfz = df1.join(df2, [\"xy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Origin|Dep Delay|\n",
      "+------+---------+\n",
      "|   ILG|   -2.407|\n",
      "|   FOE|   -1.316|\n",
      "|   LIH|     0.25|\n",
      "|   EAU|    0.808|\n",
      "|   PIE|    0.917|\n",
      "|   APF|     1.02|\n",
      "|   FAY|    1.094|\n",
      "|   TRI|    1.591|\n",
      "|   PFN|    1.594|\n",
      "|   ILM|     1.74|\n",
      "|   HSV|    1.828|\n",
      "|   ATW|    1.925|\n",
      "|   OAJ|    2.013|\n",
      "|   CHA|    2.015|\n",
      "|   CSG|    2.028|\n",
      "|   AZO|    2.095|\n",
      "|   SAV|    2.335|\n",
      "|   PHF|    2.367|\n",
      "|   SCC|    2.624|\n",
      "|   VPS|    2.664|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What was the average departure delay for each state?\n",
    "\n",
    "flights.groupBy(\"Origin\").agg(round(avg(\"DepDelay\"), 3).alias(\"Dep Delay\")).sort(\"Dep Delay\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|UniqueCarrier|        Dep delays|\n",
      "+-------------+------------------+\n",
      "|           PS| 13.31959242658045|\n",
      "|           CO|12.620661609706707|\n",
      "|           EA| 8.832762739602341|\n",
      "|           WN| 8.464397496087637|\n",
      "|           AS| 8.302911613420791|\n",
      "+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "PS has the highest average delay at 13.319\n",
      "\n",
      "\n",
      "+-------------+---------+------+------------------+\n",
      "|UniqueCarrier|Cancelled| Total|Pct% cancellations|\n",
      "+-------------+---------+------+------------------+\n",
      "|           EA|     2454|108776|              2.26|\n",
      "|           UA|     3436|152624|              2.25|\n",
      "|           PS|      879| 41706|              2.11|\n",
      "|           CO|     2508|123002|              2.04|\n",
      "|           TW|     1243| 69650|              1.78|\n",
      "+-------------+---------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "EA had the highest percentage of cancellations and would be rated as the worst airline on this front.\n"
     ]
    }
   ],
   "source": [
    "#Which airline performed the worst seen from a customer perspective? (This can be answered in many different ways. \n",
    "\n",
    "#Considering arrival delays as a decision parameter\n",
    "\n",
    "flights.groupBy(\"UniqueCarrier\").agg(avg(\"DepDelay\").alias(\"Dep delays\")).sort(desc(\"Dep delays\")).show(5)\n",
    "print(\"PS has the highest average delay at 13.319\\n\\n\")\n",
    "\n",
    "#Worst flights based on cancellations\n",
    "cancelled = flights.select(\"UniqueCarrier\").where(\"Cancelled==1\").groupBy(\"UniqueCarrier\").count().alias(\"cancelled\")\n",
    "total = flights.select(\"UniqueCarrier\").groupBy(\"UniqueCarrier\").count().alias(\"total\")\n",
    "\n",
    "\n",
    "dfx = cancelled.withColumnRenamed(\"count\", \"Cancelled\").join(total.withColumnRenamed(\"count\", \"Total\"), \"UniqueCarrier\")\n",
    "dfx = dfx.withColumn(\"Pct% cancellations\", expr(\"round((Cancelled/Total)*100, 2)\")).sort(desc(\"Pct% cancellations\"))\n",
    "dfx.show(5)\n",
    " \n",
    "print(\"EA had the highest percentage of cancellations and would be rated as the worst airline on this front.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Origin|        Dep delays|\n",
      "+------+------------------+\n",
      "|   PIR|              85.0|\n",
      "|   GUC|22.884615384615383|\n",
      "|   ACV| 19.36873156342183|\n",
      "|   MFR|17.509225092250922|\n",
      "|   EUG|16.100617828773167|\n",
      "+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Which airport performed the worst seen from a customer perspective? (This can also be answered in many different ways. \n",
    "#Describe what you consider.)\n",
    "\n",
    "flights.groupBy(\"Origin\").agg(avg(\"DepDelay\").alias(\"Dep delays\")).sort(desc(\"Dep delays\")).show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|1987|   10|        14|        3|    741|       730|    912|       849|           PS|     1451|     NA|               91|            79|     NA|      23|      11|   SAN| SFO|     447|    NA|     NA|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flights.show(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
